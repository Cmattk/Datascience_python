{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c9f967-5366-42fb-95dd-f49d0c70fd58",
   "metadata": {},
   "source": [
    "Imbalanced data pertains to datasets where the distribution of observations in the target class is uneven. In other words, one class label has a significantly higher number of observations, while the other has a notably lower count.\n",
    "\n",
    "Machine learning models may become biased in their predictions as a result, favoring the majority class. Techniques like oversampling the minority class or undersampling the majority class are used in resampling to remedy this.\n",
    "\n",
    "it is possible to evaluate model performance more precisely by substituting other assessment measures, such as precision, recall, or F1-score, for accuracy.\n",
    "\n",
    "To further improve the handling of imbalanced datasets for more reliable and equitable predictions, specialized techniques such as ensemble approaches and the incorporation of synthetic data generation can be used.\n",
    "\n",
    "Addressing imbalanced data in classification is crucial for fair model performance. Techniques include resampling (oversampling or undersampling), synthetic data generation, specialized algorithms, and alternative evaluation metrics. Implementing these strategies ensures more accurate and unbiased predictions across all classes.\n",
    "\n",
    "1. Different Evaluation Metric\n",
    "\n",
    "Classifier *accuracy* is calculated by dividing the total correct predictions by the overall predictions, suitable for balanced classes but less effective for imbalanced datasets.\n",
    "\n",
    "**Precision** gauges the accuracy of a classifier in predicting a specific class.\n",
    "**recall** assesses classifier's ability to correctly identify a class.\n",
    "\n",
    "In imbalanced datasets, the **F1 score** emerges as a preferred metric, striking a balance between precision and recall, providing a more comprehensive evaluation of a classifier’s performance.\n",
    "It can be expressed as the mean of recall and accuracy.\n",
    "F1 = 2 ∗ (precision*recall/precision+recall)\n",
    "\n",
    "Precision and F1 score both decrease when the classifier incorrectly predict the minority class, increasing the number of false positives.\n",
    "\n",
    "Recall and F1 score also drop if the classifier have trouble accurately identifying the minority class, leading to more false negatives. \n",
    "\n",
    "In particular, the F1 score only becomes better when the amount and accuracy of predictions get better.\n",
    "\n",
    "F1 score is essentially a comprehensive statistic that takes into account the trade-off between precision and recall, which is critical for assessing classifier performance in datasets that are imbalanced.***(check worldQuant's datascience lab-Bankruptcy in poland project for better understanding.)***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef2f6e-5feb-4c78-9552-b55afba984e0",
   "metadata": {},
   "source": [
    "2. Resampling (Undersampling and Oversampling)\n",
    "This method involves adjusting the balance between minority and majority classes through upsampling or downsampling.\n",
    "\n",
    "In the case of an imbalanced dataset, oversampling the minority class with replacement, termed oversampling, is employed. \n",
    "\n",
    "Conversely, undersampling entails randomly removing rows from the majority class to align with the minority class.\n",
    "\n",
    "Achieving a similar number of records for both classes in the dataset signifies that the classifier will assign equal importance to each class during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91f5e5c-72f3-4471-8bdf-69f39ee9d741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db430314-d0c1-4120-8ef5-0251d7e7af48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({1: 900, 0: 100})\n",
      "Oversampled class distribution: Counter({1: 900, 0: 900})\n",
      "Undersampled class distribution: Counter({0: 100, 1: 100})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    " \n",
    "print(\"Original class distribution:\", Counter(y))\n",
    "\n",
    "# Oversampling using RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_over, y_over = oversample.fit_resample(X, y)\n",
    "print(\"Oversampled class distribution:\", Counter(y_over))\n",
    "\n",
    "# Undersampling using RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "print(\"Undersampled class distribution:\", Counter(y_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f71c8-96d5-48bd-8a2f-74649ea85e54",
   "metadata": {},
   "source": [
    "# Assistant\n",
    "This code creates an imbalanced dataset using scikit-learn's `make_classification`. Here's what each parameter does:\n",
    "\n",
    "1. `n_classes=2`: Creates a binary classification dataset\n",
    "2. `class_sep=2`: Sets the separation between classes\n",
    "3. `weights=[0.1, 0.9]`: Creates imbalanced classes - 10% for class 0, 90% for class 1\n",
    "4. `n_informative=3`: Generates 3 informative features\n",
    "5. `n_redundant=1`: Adds 1 redundant feature\n",
    "6. `flip_y=0`: No label noise\n",
    "7. `n_features=20`: Total number of features\n",
    "8. `n_clusters_per_class=1`: Each class has 1 cluster\n",
    "9. `n_samples=1000`: Creates 1000 samples\n",
    "10. `random_state=42`: Sets seed for reproducibility\n",
    "\n",
    "The `Counter(y)` prints the distribution of classes in the generated dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed22e2-2ca6-4783-a8d6-149d431f6bb5",
   "metadata": {},
   "source": [
    "3. BalancedBaggingClassifier\n",
    "\n",
    "Traditional classifiers tend to favor the majority class, neglecting the minority class due to its lower representation.\n",
    "\n",
    "The BalancedBaggingClassifier, an extension of sklearn classifiers, addresses this imbalance by incorporating additional balancing during training. It introduces parameters like “sampling_strategy,” determining the type of resampling (e.g., ‘majority’ for resampling only the majority class, ‘all’ for resampling all classes), and “replacement,” dictating whether the sampling should occur with or without replacement. This classifier ensures a more equitable treatment of classes, particularly beneficial when handling imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da6180a2-03df-4049-bd91-ccbf3ac1e24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis code demonstrates the usage of a BalancedBaggingClassifier from the imbalanced-learn library to handle imbalanced datasets. \\nIt creates an imbalanced dataset, splits it, and trains a Random Forest classifier with balanced bagging, \\nassessing the model’s performance through accuracy and a classification report.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\"\"\"\n",
    "This code demonstrates the usage of a BalancedBaggingClassifier from the imbalanced-learn library to handle imbalanced datasets. \n",
    "It creates an imbalanced dataset, splits it, and trains a Random Forest classifier with balanced bagging, \n",
    "assessing the model’s performance through accuracy and a classification report.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dae11d70-4eeb-4f3c-955a-610601c9e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    " \n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc08b9c3-cc9f-49e5-9adb-a25d90621c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Classifier (you can use any classifier)\n",
    "base_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a BalancedBaggingClassifier\n",
    "balanced_bagging_classifier = BalancedBaggingClassifier(\n",
    "    base_classifier,\n",
    "    sampling_strategy='auto',  # You can adjust this parameter\n",
    "    replacement=False,  # Whether to sample with or without replacement\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "balanced_bagging_classifier.fit(X_train, y_train)\n",
    " \n",
    "# Make predictions\n",
    "y_pred = balanced_bagging_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75f37ca5-9fe9-4eee-bb0d-31b80b1d2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       1.00      1.00      1.00       187\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cadf0b-7d16-4415-8644-bc9c0de411b8",
   "metadata": {},
   "source": [
    "# Assistant\n",
    "Let me explain the selected code which creates and configures a BalancedBaggingClassifier:\n",
    "\n",
    "1. `base_classifier = RandomForestClassifier(random_state=42)` - Creates a Random Forest classifier as the base estimator with a fixed random seed for reproducibility.\n",
    "\n",
    "2. `balanced_bagging_classifier = BalancedBaggingClassifier(...)` - Initializes a BalancedBaggingClassifier with the following parameters:\n",
    "   - `base_classifier`: Uses the previously created Random Forest as the base estimator\n",
    "   - `sampling_strategy='auto'`: Automatically determines the sampling ratio to balance classes\n",
    "   - `replacement=False`: Samples are drawn without replacement\n",
    "   - `random_state=42`: Sets a fixed random seed for reproducible results\n",
    "\n",
    "This classifier combines bagging (bootstrap aggregating) with random under-sampling to handle imbalanced datasets by creating multiple balanced subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2285d491-510c-4816-a27d-d2ed3af4b67e",
   "metadata": {},
   "source": [
    "4. SMOTE\n",
    "The Synthetic Minority Oversampling Technique (SMOTE) addresses imbalanced datasets by synthetically generating new instances for the minority class. Unlike simply duplicating records, SMOTE enhances diversity by creating artificial instances. In\n",
    "\n",
    "simpler terms, SMOTE examines instances in the minority class, selects a random nearest neighbor using k-nearest neighbors, and generates a synthetic instance randomly within the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "149872ff-70a2-41ef-9a2c-eda06f95722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c606ea95-7820-46a5-a0eb-5974c85bf9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE: Counter({1: 713, 0: 87})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE: Counter({1: 713, 0: 713})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    " \n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\"\"\" \n",
    "# Display class distribution before SMOTE\n",
    "print(\"Class distribution before SMOTE:\", Counter(y_train))\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_train_resampled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0fd824-c5da-4e01-a110-185c39ad4475",
   "metadata": {},
   "source": [
    "5. Threshold Moving\n",
    "In classifiers, predictions are often expressed as probabilities of class membership. The conventional threshold for assigning predictions to classes is typically set at 0.5. However, in the context of imbalanced class problems, this default threshold may not yield optimal results.\n",
    "To enhance classifier performance, it is essential to adjust the threshold to a value that efficiently discriminates between the two classes.\n",
    "\n",
    "Techniques such as ROC Curves and Precision-Recall Curves are employed to identify the optimal threshold. Additionally, grid search methods or exploration within a specified range of values can be utilized to pinpoint the most suitable threshold for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8edc1d1d-2f08-4e8a-9c03-69466e2d2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "784f6869-186d-4c50-be30-e2414012df40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.50, F1 Score: 1.0000\n",
      "Threshold: 0.48, F1 Score: 1.0000\n",
      "Threshold: 0.46, F1 Score: 1.0000\n",
      "Threshold: 0.44, F1 Score: 1.0000\n",
      "Threshold: 0.42, F1 Score: 1.0000\n",
      "Threshold: 0.40, F1 Score: 1.0000\n",
      "Threshold: 0.38, F1 Score: 1.0000\n",
      "Threshold: 0.36, F1 Score: 1.0000\n",
      "Threshold: 0.34, F1 Score: 1.0000\n",
      "Threshold: 0.32, F1 Score: 1.0000\n",
      "Threshold: 0.30, F1 Score: 1.0000\n",
      "Threshold: 0.28, F1 Score: 0.9973\n",
      "Threshold: 0.26, F1 Score: 0.9973\n",
      "Threshold: 0.24, F1 Score: 0.9973\n",
      "Threshold: 0.22, F1 Score: 0.9947\n",
      "Threshold: 0.20, F1 Score: 0.9947\n",
      "Threshold: 0.18, F1 Score: 0.9947\n",
      "Threshold: 0.16, F1 Score: 0.9920\n",
      "Threshold: 0.14, F1 Score: 0.9920\n",
      "Threshold: 0.12, F1 Score: 0.9894\n",
      "Threshold: 0.10, F1 Score: 0.9842\n",
      "Threshold: 0.08, F1 Score: 0.9740\n",
      "Threshold: 0.06, F1 Score: 0.9664\n",
      "Threshold: 0.04, F1 Score: 0.9664\n",
      "Threshold: 0.02, F1 Score: 0.9664\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, class_sep=2, weights=[0.1, 0.9],\n",
    "                           n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           n_features=20, n_clusters_per_class=1,\n",
    "                           n_samples=1000, random_state=42)\n",
    " \n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    " \n",
    "\"\"\"\n",
    "# Train a classification model (Random Forest as an example)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Set a threshold (initially 0.5)\n",
    "threshold = 0.5\n",
    "\n",
    "# Adjust threshold based on your criteria (e.g., maximizing F1-score)\n",
    "while threshold >= 0:\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    " \n",
    "    print(f\"Threshold: {threshold:.2f}, F1 Score: {f1:.4f}\")\n",
    " \n",
    "    # Move the threshold (you can customize the step size)\n",
    "    threshold -= 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e98549-e79a-42ad-bada-b79e3a97e26a",
   "metadata": {},
   "source": [
    "# Assistant\n",
    "\n",
    "Probability Prediction:\n",
    "Gets probability predictions for the test set\n",
    "[:, 1] selects probabilities for the positive class (class 1)\n",
    "\n",
    "\n",
    "Threshold Testing Loop:\n",
    "   \n",
    "   - Starts with threshold of 0.5\n",
    "   - Converts probabilities to binary predictions based on threshold\n",
    "   - Calculates F1 score for each threshold\n",
    "   - Decrements threshold by 0.02 in each iteration\n",
    "   - Prints threshold and corresponding F1 score\n",
    "\n",
    "This code explores different classification thresholds to find the optimal decision boundary for an imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5249e2-fca3-4f1f-b22a-9e574a41623f",
   "metadata": {},
   "source": [
    "6. Using Tree Based Models\n",
    "The hierarchical structure of tree-based models—such as Decision Trees, Random Forests, and Gradient Boosted Trees—allows them to handle imbalanced datasets better than non-tree-based models.\n",
    "\n",
    "Decision Trees: \n",
    "Decision trees create a structure resembling a tree by dividing the feature space into regions according to feature values. By changing the decision boundaries to incorporate minority class patterns, decision trees can react to data that is unbalanced. They might experience overfitting, though.\n",
    "\n",
    "Random Forests: \n",
    "Random Forests are made up of many Decision Trees that have been trained using arbitrary subsets of features and data. Random Forests improve generalization by reducing overfitting and strengthening robustness against imbalanced datasets by mixing numerous trees.\n",
    "\n",
    "Gradient Boosted Trees: \n",
    "Boosted Gradient Trees grow in a sequential fashion, with each new growth repairing the mistakes of the older one. Gradient Boosted Trees perform well in imbalanced circumstances because of their ability to concentrate on misclassified occurrences through sequential learning. Although they often work effectively, they could be noise-sensitive.\n",
    "\n",
    "\n",
    "7. Using Anomaly Detection Algorithms\n",
    "Anomaly or Outlier Detection algorithms are ‘one class classification algorithms’ that helps in identifying outliers ( rare data points) in the dataset.\n",
    "In an Imbalanced dataset, assume  ‘Majority class records as Normal data’ and ‘Minority Class records as Outlier data’.\n",
    "These algorithms are trained on Normal data.\n",
    "A trained model can predict if the new record is Normal or Outlier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
